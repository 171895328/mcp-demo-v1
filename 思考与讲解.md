# MCP_DEMO说明文档

## 目录
- [1. MCP协议简介](#1-mcp协议简介)
  - [1.1 MCP的核心思想](#11-mcp的核心思想)
  - [1.2 主要组件](#12-主要组件)
  - [1.3 MCP提供的功能](#13-mcp提供的功能)
  - [1.4 为什么MCP很重要？](#14-为什么mcp很重要)
  - [1.5 实际应用](#15-实际应用)
  - [1.6 总结](#16-总结)
- [2. MCP服务端简介](#2-mcp服务端简介)
  - [2.1 导入必要的库](#21-导入必要的库)
  - [2.2 创建MCP服务实例](#22-创建mcp服务实例)
  - [2.3 注册工具函数](#23-注册工具函数)
  - [2.4 启动MCP服务](#24-启动mcp服务)
- [3. MCP客户端代码分析与介绍](#3-mcp客户端代码分析与介绍)
  - [3.1 MCP客户端的任务流程](#31-mcp客户端的任务流程)
  - [3.2 代码结构与MCP流程的对应关系](#32-代码结构与mcp流程的对应关系)
  - [3.3 关键实现：对话管理与优化](#33-关键实现对话管理与优化)
  - [3.4 工具调用的迭代流程](#34-工具调用的迭代流程)
  - [3.5 资源和提示的使用](#35-资源和提示的使用)
- [4. MCP应用使用示例](#4-mcp应用使用示例)
  - [4.1 基于原文自动构建知识图谱](#41-基于原文自动构建知识图谱)
  - [4.2 MongoDB数据库操作示例](#42-mongodb数据库操作示例)
- [5. 当前面临的问题与解决思路](#5-当前面临的问题与解决思路)
  - [5.1 问题1：工具信息过载](#51-问题1工具信息过载)
  - [5.2 问题2：上下文窗口溢出](#52-问题2上下文窗口溢出)
  - [5.3 问题3：工具调用格式问题](#53-问题3工具调用格式问题)
- [6. MCP客户端的实际工作流程示例](#6-mcp客户端的实际工作流程示例)
- [7. 总结](#7-总结)

## 1. MCP协议简介

模型上下文协议（Model Context Protocol，简称MCP）是一个开放的协议，它为大型语言模型（LLM）应用和外部数据源、工具之间的无缝集成提供了标准化方法。无论你是构建AI驱动的IDE、增强聊天界面，还是创建自定义AI工作流，MCP都提供了一种标准化方式来连接LLM与它们所需的上下文。

### 1.1 MCP的核心思想

MCP使用客户端-主机-服务器架构，每个主机可以运行多个客户端实例。这种架构使用户能够在应用程序之间集成AI功能，同时保持明确的安全边界和关注点隔离。基于JSON-RPC构建，MCP提供了一个有状态的会话协议，专注于客户端和服务器之间的上下文交换和采样协调。

### 1.2 主要组件

1. **主机（Host）**：作为容器和协调器
   - 创建和管理多个客户端实例
   - 控制客户端连接权限和生命周期
   - 强制执行安全策略和同意要求
2. **客户端（Clients）**：由主机创建，维护与服务器的连接
   - 与每个服务器建立一对一的有状态会话
   - 处理协议协商和能力交换
   - 双向路由协议消息
3. **服务器（Servers）**：提供专门的上下文和功能
   - 通过MCP原语公开资源、工具和提示
   - 独立运行，具有明确的责任
   - 可以是本地进程或远程服务

![image-20250418182223434](C:\Users\wasd\AppData\Roaming\Typora\typora-user-images\image-20250418182223434.png)

### 1.3 MCP提供的功能

MCP为应用程序提供了一种标准化的方式来：

1. **资源（Resources）**：与语言模型共享上下文信息
   - 文件内容、数据库记录、API响应等

2. **工具（Tools）**：向AI系统暴露工具和功能
   - 函数调用、API访问、计算能力等

3. **提示（Prompts）**：预定义的模板或指令
   - 指导语言模型交互的结构化消息

4. **采样（Sampling）**：服务器可以请求通过客户端进行LLM生成

### 1.4 为什么MCP很重要？

MCP就像AI应用程序的USB-C接口。就像USB-C为物理设备提供了一个通用适配器一样，MCP为AI应用提供了一种标准化方式来连接不同的数据和工具。

在MCP出现之前，开发人员必须为他们希望AI应用连接的每个数据源或工具构建自定义连接，这是一个耗时的过程，通常导致功能有限。现在，有了MCP，开发人员可以轻松地向他们的AI应用添加连接，大大减少了开发成本。

### 1.5 实际应用

例如，MCP允许：

- Claude桌面版读取你的本地文件系统
- 代码编辑器从Git仓库获取上下文
- 数据分析工具连接到数据库
- 客户服务机器人访问公司知识库

### 1.6 总结

MCP通过提供一个标准化的协议，使AI应用能够访问它们需要的上下文信息，从而使它们更加有用和强大。它的模块化设计允许实现支持正好他们需要的特性，同时强大的功能集使复杂集成成为可能。这使得开发人员可以构建更智能、更有用的AI应用，能够在自然语言对话中无缝集成外部数据和工具。

## 2. MCP服务端简介

以一个能获取天气信息的MCP服务为例解释如何构建MCP服务及工具

### 2.1 导入必要的库

首先，需要导入FastMCP库和其他必要的依赖：

```python
from mcp.server import FastMCP
import json
import httpx  # 用于HTTP请求
```

### 2.2 创建MCP服务实例

创建一个MCP服务实例，并为其指定名称：

```python
mcp = FastMCP("WeatherServer")
```

这一步创建了一个名为"WeatherServer"的MCP服务实例，后续所有工具都将注册到这个实例中。

### 2.3 注册工具函数

MCP使用装饰器方式注册工具函数，这是整个服务的核心部分：

```python
@mcp.tool()
async def get_weather_tool(city: str) -> str:
    """获取指定城市的天气信息
    
    Args:
        city: 城市名称
        
    Returns:
        格式化后的天气信息字符串
    """
    weather_data = await get_weather(city)
    return format_weather_data(weather_data)
```

注册工具函数时需要注意以下几点：

1. 使用`@mcp.tool()`装饰器标记函数为工具
2. 明确定义参数类型和返回值类型
3. 编写详细的文档字符串，包括:
   - 功能描述
   - 参数说明
   - 返回值说明

文档字符串非常重要，它将用于生成工具的使用说明。

### 2.4 启动MCP服务

最后，启动MCP服务：

```python
if __name__ == "__main__":
    mcp.run(transport="stdio")
```

## 3. MCP客户端代码分析与介绍

### 3.1 MCP客户端的任务流程

理解这段代码的关键是理解整个MCP客户端的任务流程。首先从一个完整的用户交互开始，梳理整个流程：

1. **用户输入查询** → 客户端接收用户输入，添加到对话历史
2. **LLM决策** → 将用户查询和工具信息发送给LLM
3. **分支决策** → 让LLM决定是回答问题还是使用工具
4. **工具调用** → 如果LLM决定使用工具，客户端解析工具调用请求并执行
5. **结果处理** → 将工具执行结果发送回LLM，让LLM分析结果
6. **二次决策** → LLM基于工具结果决定是否需要更多信息（使用更多工具）或生成回答
7. **最终响应** → 将LLM的最终回答呈现给用户

这个流程的4-6步骤可能会重复多次，直到LLM认为已经收集了足够的信息可以回答用户。

下面，我们将基于这个流程，详细解释代码的实现。

### 3.2 代码结构与MCP流程的对应关系

#### 3.2.1 配置加载 - MCPClientConfig类

首先，客户端需要加载配置信息，知道如何连接到LLM和MCP服务器：

```python
import asyncio
import json
import os
import traceback
import re
from typing import Dict, List, Any
from contextlib import AsyncExitStack
import requests
from dotenv import load_dotenv

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from utils.handleLog import log_llm_summary,log_tool_info
from utils.TokenAndConversation import TokenCounter, ConversationManager

class MCPClientConfig:
    """处理MCP客户端配置的类"""
    
    def __init__(self):
        # 加载模型配置
        load_dotenv('../model.env')
        self.model = os.getenv('MODEL')
        self.model_base_url = os.getenv('MODEL_BASE_URL')
        self.model_api_key = os.getenv('MODEL_API_KEY')
        self.model_contextWindow = int(os.getenv('MODEL_API_OPTION_CONTEXTWINDOWS'))
        # 加载服务器配置
        self.server_config = self._load_server_config()
```

这个类从两个地方加载配置：
- `model.env`文件：包含LLM的配置信息

  ```
  MODEL=qwen2.5:32b
  MODEL_BASE_URL=http://localhost:6006/api/chat
  MODEL_API_KEY=ollama
  MODEL_API_OPTION_CONTEXTWINDOWS=16000
  ```

- `mcp_config.json`文件：包含MCP服务器的配置信息

  例如天气的MCP服务器的配置信息如下：

  ```json
  {
      "servers": {
          "weather": {
              "command": "python",
              "args": ["../mcp_servers/weather-server.py"],
              "env": null
          }
  }
  ```

这是整个流程的准备阶段，确保客户端知道如何连接到各个组件。

#### 3.2.2 服务器连接 - connect_to_servers方法

客户端启动后，首先需要连接到所有配置的MCP服务器：

```python
async def connect_to_servers(self) -> None:
    """连接到所有配置的MCP服务器"""
    for server_name, server_config in self.config.server_config.get("servers", {}).items():
        try:
            server_params = StdioServerParameters(**server_config)
            stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
            stdio, write = stdio_transport
            session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))
            await session.initialize()
            self.sessions[server_name] = session
            
            # 这里列出服务器上的工具、资源和提示
            # ...
```

这个方法使用MCP协议的客户端实现（`stdio_client`和`ClientSession`）连接到服务器，并列出每个服务器上的可用工具、资源和提示。这是MCP协议的关键部分，因为它定义了客户端和服务器之间如何通信。

这一步完成了与工具服务器的连接，是工具调用的前提。

#### 3.2.3 工具格式转换 - ToolAdapter类

MCP协议定义了工具的描述格式，但LLM可能需要不同的格式。ToolAdapter类负责这种格式转换：

```python
class ToolAdapter:
    """工具格式转换和管理"""
    
    @staticmethod
    def convert_tool_format(tool):
        """将MCP工具格式转换为LLM工具调用格式"""
        properties = {}
        for name, prop in tool.inputSchema.get("properties", {}).items():
            # 转换类型等信息
            # ...
        
        return {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": {
                    "type": "object",
                    "properties": properties,
                    "required": tool.inputSchema.get("required", [])
                }
            }
        }
```

这个类还负责解析LLM返回的工具调用请求：

```python
@staticmethod
def parse_tool_calls(content):
    """解析AI返回的工具调用"""
    if "tool_calls" not in content:
        # 处理可能的格式问题
        # ...
    
    # 返回工具调用信息
    tool_calls = []
    for tool_call in content["tool_calls"]:
        tool_calls.append({
            "tool_name": tool_call["function"]["name"],
            "tool_args": tool_call["function"]["arguments"]
            # ...
        })
    return tool_calls
```

这部分实现了MCP客户端的一个关键功能：工具描述的格式转换和工具调用请求的解析。

#### 3.2.4 LLM通信 - LLMClient类

客户端需要与LLM通信，获取决策和回答：

```python
class LLMClient:
    """处理与LLM API的通信"""
    
    async def call_llm(self, messages: List[Dict], tools: List = None) -> Dict:
        """调用LLM API获取响应"""
        try:
            # 准备请求数据
            data = {
                "model": self.config.model,
                "messages": messages,
                "stream": False,
                "options": {"num_ctx": self.config.model_contextWindow}
            }
           
            if tools:
                data["tools"] = tools
            
            # 发送请求
            response = requests.post(
                self.config.model_base_url,
                data=json.dumps(data),
                headers=headers
            )
            
            #处理响应 ...
```

这个类封装了与LLM API的通信，是实现流程中"LLM决策"和"二次决策"的关键。值得注意的是，这里将工具信息（`tools`）作为可选参数传递给LLM，使LLM能够知道有哪些工具可用。

#### 3.2.5 初步判断 - process_query方法

```python
async def process_query(self, query: str) -> str:
        """处理用户查询的主方法"""
        # 重置计数器
        self.count = 0
        
        # 第一步：决定使用哪些工具或直接回答
        decision = await self.decide_next_action(query)
        
        if "error" in decision:
            return decision["error"]
            
        if "response" in decision:
            return decision["response"]
        
        # 第二步：串行处理工具调用
        return await self.process_tool_result(decision["tool_calls"])
```

这个方法初步判断用户需求是否需要进行工具调用。如果不需要则直接生成回复，否则生成工具调用信息并将工具调用信息送入process_tool_result()方法执行下一步流程。

#### 3.2.6 决策流程 - decide_next_action和process_tool_result方法

这两个方法实现了MCP客户端的核心决策流程：

```python
async def decide_next_action(self, query: str = None) -> Dict:
    """决定下一步行动：调用工具或生成最终响应"""
    # 添加用户消息到历史
    if query:
        self.conversation_manager.add_message({
            "role": "user",
            "content": query
        })
        # 添加系统提示
        # ...
    
    # 获取可用工具
    available_tools = await self.get_available_tools()
    
    # 获取当前消息历史
    current_messages = self.conversation_manager.get_current_messages()
    
    # 获取AI响应
    response_data = await self.llm_client.call_llm(current_messages, available_tools)
    
    # 解析响应
    content = response_data["message"]
    self.conversation_manager.add_message(content)
    
    # 解析工具调用
    tool_calls = ToolAdapter.parse_tool_calls(content)
    
    if not tool_calls:
        return {"response": content["content"]}
    
    return {"tool_calls": tool_calls}
```

这个方法实现了流程中的"LLM决策"部分，它将用户查询和可用工具信息发送给LLM，然后解析LLM的响应，判断是直接回答还是调用工具。

```python
async def process_tool_result(self, tool_calls: List[Dict]) -> str:
    """串行处理工具调用并生成最终响应"""
    iteration = 0
    pending_calls = tool_calls.copy()
    
    while pending_calls and iteration < self.max_tool_iterations:
        iteration += 1
        
        # 每次处理一个工具调用
        current_call = pending_calls.pop(0)
        
        # 执行工具调用
        # ...获取到了工具执行的结果tool_content
        
        # 添加工具结果到历史
        tool_result_msg = {
            "role": "tool",
            "name": tool_name,
            "content": str(tool_content)
        }
        self.conversation_manager.add_message(tool_result_msg, is_key_message=True)
        
        # 添加系统提示引导决策
        # ...
        
        # 获取下一步决策
        decision = await self.decide_next_action()
        
        if "tool_calls" in decision:
            # 新工具调用添加到队列
            pending_calls.extend(decision["tool_calls"])
        elif "response" in decision:
            # 返回最终响应
            return decision["response"]
    
    # 如果达到最大迭代次数或没有更多工具调用，生成最终响应
    # ...
```

这个方法实现了流程中的"工具调用"、"结果处理"和"二次决策"部分，它串行处理工具调用，并将结果返回给LLM进行分析，直到LLM认为已经收集了足够的信息可以回答用户。

#### 3.2.7 工具调用执行 - call_tool_with_timeout方法

当LLM决定调用工具时，客户端需要执行工具调用：

```python
async def call_tool_with_timeout(self, session, tool_name, args, timeout=30) -> Any:
    """带超时的工具调用"""
    try:
        return await asyncio.wait_for(
            session.call_tool(tool_name, args),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        print(f"工具 {tool_name} 调用超时")
        return None
    except Exception as e:
        print(f"工具 {tool_name} 调用出错: {str(e)}")
        return None
```

这个方法使用MCP协议的`call_tool`方法执行工具调用，并设置了超时机制，确保工具调用不会无限期地阻塞程序。这是实现"工具调用"的具体方法。

#### 3.2.8 用户交互 - chat_loop方法

最后，客户端需要提供用户交互界面：

```python
async def chat_loop(self):
    """运行交互式聊天循环"""
    print("\n优化版MCP客户端已启动！")
    print("输入查询内容, 或者使用以下特殊命令:")
    # ...

    while True:
        try:
            query = input(f"\n{self.prompt}").strip()
            
            # 处理特殊命令
            if query.lower() == 'quit':
                break
            # 其他命令处理
            # ...
            
            # 处理普通查询
            result = await self.process_query(query)
            
            print("AI回答:")
            print(result)
        except Exception as e:
            print(f"处理查询时发生错误: {str(e)}")
```

这个方法实现了用户交互，它接收用户输入，处理特殊命令，或者调用`process_query`方法处理普通查询。`process_query`方法是整个流程的入口，它调用`decide_next_action`和`process_tool_result`方法，实现完整的对话流程。

### 3.3 关键实现：对话管理与优化

在整个MCP客户端中，对话管理是一个关键部分，它确保对话历史不会超出LLM的上下文窗口限制：

```python
# 初始化对话管理器
self.conversation_manager = ConversationManager(
    model_api_url=self.config.model_base_url,
    model_name=self.config.model,
    token_counter=self.token_counter
)

# 优化历史记录
await self.conversation_manager.optimize_history()
```

这个`ConversationManager`类负责：
- 添加和管理消息
- 计算令牌数量
- 标记关键消息
- 自动优化历史记录，确保在令牌限制内

这是实现长对话的关键，因为它解决了LLM上下文窗口有限的问题。

### 3.4 工具调用的迭代流程

MCP客户端实现了一种迭代的工具调用流程：

1. LLM分析用户需求
2. 是否决定调用工具A
3. 客户端执行工具A，获取结果
4. LLM分析结果，决定是否需要调用工具B
5. 如需要，客户端执行工具B，获取结果
6. 重复2-5步骤直至不需要再使用工具或是重复次数的上限
7. LLM决定已经收集了足够信息，生成最终回答

这种实现允许LLM进行复杂的多步骤推理，使用多个工具来解决用户问题。这是通过`process_tool_result`方法中的迭代循环实现的：

```python
while pending_calls and iteration < self.max_tool_iterations:
    # 处理当前工具调用
    # ...
    
    # 获取下一步决策
    decision = await self.decide_next_action()
    
    if "tool_calls" in decision:
        # 有新的工具调用
        pending_calls.extend(decision["tool_calls"])
    elif "response" in decision:
        # 返回最终响应
        return decision["response"]
```

系统添加了一个重要的系统提示来引导LLM的决策：

```python
temp_system_msg = {
    "role": "system-tip",
    "content": "请审查最近的用户消息和之后的所有消息，判断是否有足够的信息完成用户需求，如果还需要继续调用工具，请不要生成方案，而是继续调用工具"
}
```

这个提示引导LLM判断是否需要继续调用工具，而不是过早地生成最终回答。

### 3.5 资源和提示的使用

除了工具调用，MCP客户端还支持资源和提示的使用：

```python
async def handle_resource_command(self, args):
    """处理资源命令"""
    # ...
    if action == 'get':
        # 获取资源内容
        result = await self.sessions[server_name].read_resource(actual_resource_name)
        # ...
    elif action == 'use':
        # 添加资源到对话上下文
        result = await self.sessions[server_name].read_resource(actual_resource_name)
        self.conversation_manager.add_message({
            "role": "system",
            "content": f"以下是资源 {resource_name} 的内容:\n{resource_content}"
        }, is_key_message=True)
        # ...
```

```python
async def handle_prompt_command(self, args):
    """处理提示命令"""
    # ...
    # 执行提示
    result = await self.sessions[server_name].get_prompt(actual_prompt_name, params)
    # 添加提示结果到对话上下文
    self.conversation_manager.add_message({
        "role": "system",
        "content": f"以下是执行提示 {prompt_name} 的结果:\n{prompt_result}"
    }, is_key_message=True)
    # ...
```

这些方法允许用户获取资源内容，或者将资源和提示的结果添加到对话上下文中，为LLM提供更多信息。这是MCP协议的另一个关键特性，因为它允许客户端访问服务器上的各种资源和预定义提示。

## 4. MCP应用使用示例

### 4.1 基于原文自动构建知识图谱

这段展示了MCP应用如何自动读取一篇包含科技公司创始人信息的文本，并生成结构化的知识图谱。

#### 4.1.1 原始文本内容

> 华为技术有限公司是中国领先的通信技术公司，总部位于深圳。该公司由任正非于1987年创立，最初专注于电话交换设备的研发与销售。经过多年的发展，华为已成为全球最大的通信设备制造商之一，同时在智能手机、云计算、人工智能等领域积极布局。2019年，美国政府将华为列入实体清单，限制其与美国公司的业务往来。尽管面临国际压力，华为依然在5G网络建设方面保持全球领先地位。
>
> 除了华为之外，小米公司也是中国科技产业的重要成员。小米由雷军和林斌于2010年在北京创立，最初以"性价比高"的智能手机闻名。随着业务的拓展，小米逐渐进入智能家居、电视、笔记本电脑等多个领域。2021年，小米宣布将进军电动汽车行业，并计划投资100亿美元。雷军在公开演讲中表示，小米的目标是打造"智能生态链"，让万物互联成为现实。
>
> 此外，字节跳动是一家以内容平台起家的互联网公司，成立于2012年，总部位于北京。其最著名的产品是抖音和海外版TikTok，深受全球年轻用户喜爱。公司创始人张一鸣在2021年宣布卸任CEO，由梁汝波接任。字节跳动也在人工智能和教育科技等领域不断扩张，例如推出智能学习平台"瓜瓜龙"，以及企业协作工具"飞书"。

#### 4.1.2 MCP应用交互

```plaintext
>> 读取这个文本的内容：C:\Users\wasd\Desktop\myfolder\知识图谱原文.txt,然后为我生成一份详细的知识图谱，要求包含实体的节点以及实体间关系的节点 

AI决定使用工具 (1/10): {'tool_name': 'file-system.read_file', 'tool_args': {'path': 'C:\\Users\\wasd\\Desktop\\myfolder\\知识图谱原文.txt'}}
工具执行结果为: 华为技术有限公司是中...（完整结果已记录到日志）

AI决定使用工具 (2/10): {'tool_name': 'memory.create_entities', 'tool_args': {'entities': ['华为技术有限公司', '小米公司', '字节跳动', '任正非', '雷军', '林斌', '张一鸣', '梁汝波']}}
工具执行结果为: [
  "华为技术有...（完整结果已记录到日志）

AI决定使用工具 (3/10): {'tool_name': 'memory.create_relations', 'tool_args': {'relations': [['创建了', '任正非', '华为技术有限公司'], ['共同创建了', '雷军', '小米公司'], ['共同创建了', '林斌', '小米公司'], ['是创始人', '张一鸣', '字节跳动'], ['接替成为CEO', '梁汝波', '字节跳动']]}}
工具执行结果为: [
  [
    ...（完整结果已记录到日志）

AI回答:
已经根据提供的文本生成了知识图谱，其中包含了几个主要的实体节点和它们之间的关系。以下是创建的具体内容：

- 实体包括：华为技术有限公司、小米公司、字节跳动、任正非、雷军、林斌、张一鸣、梁汝波。

- 关系有：
  - "创建了" 关联的是：任正非 和 华为技术有限公司
  - "共同创建了" 关联的是：雷军, 林斌 和 小米公司
  - "是创始人" 关联的是：张一鸣 和 字节跳动
  - "接替成为CEO" 关联的是：梁汝波 和 字节跳动
```

#### 4.1.3 最终生成的知识图谱

```json
{"0":"华","1":"为","2":"技","3":"术","4":"有","5":"限","6":"公","7":"司","type":"entity"}
{"0":"小","1":"米","2":"公","3":"司","type":"entity"}
{"0":"字","1":"节","2":"跳","3":"动","type":"entity"}
{"0":"任","1":"正","2":"非","type":"entity"}
{"0":"雷","1":"军","type":"entity"}
{"0":"林","1":"斌","type":"entity"}
{"0":"张","1":"一","2":"鸣","type":"entity"}
{"0":"梁","1":"汝","2":"波","type":"entity"}
{"0":"创建了","1":"任正非","2":"华为技术有限公司","type":"relation"}
{"0":"共同创建了","1":"雷军","2":"小米公司","type":"relation"}
{"0":"共同创建了","1":"林斌","2":"小米公司","type":"relation"}
{"0":"是创始人","1":"张一鸣","2":"字节跳动","type":"relation"}
{"0":"接替成为CEO","1":"梁汝波","2":"字节跳动","type":"relation"}
```

MCP应用能够成功理解原始自然语言文本并基于原始文本生成实体节点及对应的实体间节点。但是目前为止生成的实体以及实体关系较为简单,实体和关系的数量也较少，无法生成更深层次的关系，我初步认为这一结果是与MCP应用使用的模型相关联的。

### 4.2 MongoDB数据库操作示例

这段展示了MCP应用如何理解用户对于mongodb数据库的操作的需求，并成功查询到最终结果。

```plaintext
>> 我想查看所有年龄在 20 到 25 岁之间，专业是“计算机科学”或“人工智能”的学生，他们至少选修了两门课程，并且至少有一门课程的成绩在 90 分以上。请列出这些学生的姓名、专业、所选课程的名称与成绩，以及
每个人的平均成绩，按平均成绩从高到低排序。课程名称也请用中文返回。最后将结果保存至C:\Users\wasd\Desktop\myfolder\mongodb_output.txt

AI决定使用工具 (1/10): {'tool_name': 'mongodb.aggregate', 'tool_args': {'collection': 'grades', 'pipeline': [{'$lookup': {'as': 'student_info', 'foreignField': 'student_id', 'from': 'students', 'localField': 'student_id'}}, {'$unwind': '$student_info'}, {'$match': {'$and': [{'student_info.age': {'$gte': 20}}, {'student_info.age': {'$lte': 25}}, {'$or': [{'student_info.major': '计算机科学'}, {'student_info.major': '人工智能'}]}, {'score': {'$gt': 90}}]}}, {'$lookup': {'as': 'course_name_zh', 'foreignField': 'course_id', 'from': 'courses', 'localField': 'course_id'}}, {'$unwind': '$course_name_zh'}, {'$group': {'_id': {'avg_score': {'$avg': '$score'}, 'courses': {'$push': {'name': '$course_name_zh.name', 'score': '$score'}}, 'major': '$student_info.major', 'student_name': '$student_info.name'}}}, {'$match': {'courses.1': {'$exists': True}}}, {'$sort': {'_id.avg_score': -1}}]}}
工具 aggregate 调用出错: Failed to aggregate collection grades: Unrecognized expression '$push'
工具执行结果为: 工具调用失败或超时（完整结果已记录到日志）

AI决定使用工具 (2/10): {'tool_name': 'mongodb.aggregate', 'tool_args': {'collection': 'grades', 'pipeline': [{'$lookup': {'as': 'student_info', 'foreignField': 'student_id', 'from': 'students', 'localField': 'student_id'}}, {'$unwind': '$student_info'}, {'$match': {'$and': [{'student_info.age': {'$gte': 20}}, {'student_info.age': {'$lte': 25}}, {'$or': [{'student_info.major': '计算机科学'}, {'student_info.major': '人工智能'}]}, {'score': {'$gt': 90}}]}}, {'$lookup': {'as': 'course_name_zh', 'foreignField': 'course_id', 'from': 'courses', 'localField': 'course_id'}}, {'$unwind': '$course_name_zh'}, {'$group': {'_id': {'avg_score': {'$avg': '$score'}, 'courses': {'$push': {'name': '$course_name_zh.name', 'score': '$score'}}, 'major': '$student_info.major', 'student_name': '$student_info.name'}}}, {'$match': {'courses.1': {'$exists': True}}}, {'$sort': {'_id.avg_score': -1}}]}}
工具 aggregate 调用出错: Failed to aggregate collection grades: Unrecognized expression '$push'
工具执行结果为: 工具调用失败或超时（完整结果已记录到日志）

AI回答:
看起来在尝试从数据库中获取信息时遇到了问题，可能是因为查询过于复杂导致执行时间过长或者网络连接不稳定。为了确保能够得到你需要的数据并将其保存到指定文件，我将分步进行操作以避免单一请求过于庞大。  

首先，我们将根据年龄、专业和分数条件筛选出符合条件的学生ID列表。
然后，获取每个学生的详细信息及其所修课程的名称与成绩，并计算平均成绩。
最后，将结果按照要求排序后写入指定文本文件中。让我们开始第一步：
<tool_call>
{"name": "mongodb.query", "arguments": {"collection":"grades","filter":{"$and":[{"student_id":{"$in":[ObjectId("609c834f5d12e72a24eacf8c"),ObjectId("609c834f5d12e72a24eacf8d")]}},{"score":{"$gt":90}}]},"projection":{"student_id":1,"_id":0},"limit":null,"explain":null}}
</tool_call>

>> 在 MongoDB 中，使用 $push 进行数组操作时会遇到一个问题。具体来说，$push 不允许在聚合阶段中直接使用未定义的字段。我们需要调整聚合管道，特别是如何存储课程和成绩数据的方式。修正方案是使用 $addToSet 或 $push 之前确保字段是正确地指定和计算的。下面是针对该错误的修正方案：修改 $group 阶段，确保正确处理课程与成绩，并避免直接在 $group 中使用 score 和 course_name 字段。通过重新组织数据结构来
避免 $push 的问题，确保它与 courses 关联。

AI决定使用工具 (1/10): {'tool_name': 'mongodb.query', 'tool_args': {'collection': 'students', 'explain': None, 'filter': {'$and': [{'age': {'$gte': 20}}, {'age': {'$lte': 25}}, {'$or': [{'major': '计算机科学'}, {'major': '人工智能'}]}]}, 'limit': None, 'projection': {'_id': 0, 'student_id': 1}}}
工具执行结果为: [
  {
    ...（完整结果已记录到日志）

AI决定使用工具 (2/10): {'tool_name': 'mongodb.aggregate', 'tool_args': {'collection': 'grades', 'pipeline': [{'$match': {'student_id': {'$in': ['S001', 'S006', 'S007', 'S008', 'S010', 'S013', 'S018', 'S020', 'S022', 'S023', 'S025', 'S029', 'S035', 'S036', 'S037', 'S048', 'S049', 'S050', 'S055', 'S056', 'S060', 'S064', 'S067', 'S081', 'S083', 'S084', 'S085', 'S086', 'S089', 'S091', 'S092', 'S101', 'S102', 'S103', 'S104', 'S106', 'S107', 'S108', 'S109', 'S110', 'S111', 'S112', 'S117', 'S121', 'S122', 'S124', 'S125', 'S127', 'S134', 'S136', 'S138', 'S141', 'S142', 'S145', 'S148', 'S151', 'S152', 'S153', 'S156', 'S159', 'S170', 'S171', 'S177', 'S181', 'S186', 'S188', 'S190', 'S192', 'S194', 'S196', 'S198', 'S200']}}}, {'$lookup': {'as': 'student_info', 'foreignField': 'student_id', 'from': 'students', 'localField': 'student_id'}}, {'$unwind': '$student_info'}, {'$lookup': {'as': 'course_name_zh', 'foreignField': 'course_id', 'from': 'courses', 'localField': 'course_id'}}, {'$match': {'score': {'$gt': 90}}}, {'$group': {'_id': {'major': '$student_info.major', 'name': '$student_info.name', 'student_id': '$student_id'}, 'avg_score': {'$avg': '$score'}, 'courses': {'$push': {'name': '$course_name_zh.name', 'score': '$score'}}}}, {'$match': {'courses.1': {'$exists': True}}}, {'$sort': {'avg_score': -1}}]}}
工具执行结果为: [
  {
    ...（完整结果已记录到日志）

AI决定使用工具 (3/10): {'tool_name': 'file-system.write_file', 'tool_args': {'content': '姓名, 专业, 平均成绩, 课程及分数\n阎玉兰, 计算机科学, 98.67, 编译原理(99), 软件工程(97), 计算机网络(100)\n严晶, 计算机科学, 98.5, 软件工程(97), 计算机组成原理(100)\n王杨, 人工智能, 98, 操作系统原理(98), 人工智能导论(98)\n杜云, 计算机科学, 98, 离散数学(100), 编译原理(100), 数据结构与算法(94)\n张超, 人工智能, 98, 数据库系统(97), 程序设计基础(99)\n黄秀荣, 人工智能, 97.8, 编译原理(94), 计算机组成原理(96), 操作系统原理(100), 计算机网络(100), 人工智能导论(99)\n邵建华, 计算机科学, 97.67, 离散数 学(97), 人工智能导论(99), 数据结构与算法(97)\n张红霞, 人工智能, 97.5, 计算机组成原理(96), 编译原理(99)\n张想, 计算机科学, 97.5, 编译原理(99), 操作系统原理(96)\n李佳, 计算机科学, 96.67, 数据结构 与算法(96), 人工智能导论(94), 程序设计基础(100)\n秦斌, 计算机科学, 96.67, 程序设计基础(92), 数据库系统(100), 软件工程(98)\n潘红梅, 人工智能, 96, 数据库系统(98), 操作系统原理(99), 程序设计基础(91)\n盛杨, 计算机科学, 96, 操作系统原理(98), 软件工程(94)\n黎金凤, 计算机科学, 95.75, 程序设计基础(93), 计算机组成原理(96), 数据结构与算法(100), 离散数学(94)\n许宇, 人工智能, 95.75, 离散数学(95), 计算机组成原理(98), 数据库系统(92), 程序设计基础(98)\n许春梅, 计算机科学, 95.5, 计算机组成原理(99), 操作系统原理(92)\n刘凯, 计算机科学, 95.5, 软件工程(95), 人工智能导论(96)\n王建国, 计算机科学, 95.5, 操作系统原理(100), 编译原理(91)\n周淑珍, 人工智能, 95.5, 程序设计基础(96), 软件工程(95)\n张宁, 人工智能, 95.33, 离散数学(98), 编译原理(97), 操作系统原理(91)\n赵晶, 人工智能, 95.25, 程序设 计基础(91), 离散数学(98), 编译原理(97), 计算机网络(95)\n胡雪, 计算机科学, 95, 软件工程(93), 人工智能导论(97)\n王华, 计算机科学, 95, 数据结构与算法(94), 计算机组成原理(96)\n韩旭, 人工智能, 94.75, 数据库系统(93), 操作系统原理(98), 数据结构与算法(91), 离散数学(97)\n赵建国, 人工智能, 94, 操作系统原理(92), 软件工程(94), 离散数学(96)\n邹淑珍, 计算机科学, 93.5, 数据库系统(92), 计算机组成原理(95)\n刘倩, 计算机科学, 93.5, 计算机网络(94), 离散数学(93)\n杜帅, 计算机科学, 93.5, 计算机组成原理(93), 数据库系统(94)\n史凯, 人工智能, 93.5, 数据库系统(93), 离散数学(94)\n李雷, 计算机科学, 93,  操作系统原理(96), 数据结构与算法(91), 计算机组成原理(92)\n周岩, 人工智能, 92, 程序设计基础(91), 数据库系统(93)\n朱龙, 人工智能, 92, 操作系统原理(91), 编译原理(93)\n廖刚, 计算机科学, 91, 计算机组成原理(91), 离散数学(91)', 'path': 'C:\\Users\\wasd\\Desktop\\myfolder\\mongodb_output.txt'}}
工具执行结果为: {"success"...（完整结果已记录到日志）

AI回答:
已经成功生成并保存了包含符合条件的学生信息的文件至指定路径 C:\Users\wasd\Desktop\myfolder\mongodb_output.txt。该文件列出了每个学生的姓名、专业、平均成绩以及他们所选课程的名称与分数，按照平均成绩从高到低排序。

如果您需要进一步的信息或操作，请随时告诉我！
```

#### 4.2.1 最后保存在文件中的执行结果

```plaintext
姓名, 专业, 平均成绩, 课程及分数
阎玉兰, 计算机科学, 98.67, 编译原理(99), 软件工程(97), 计算机网络(100)
严晶, 计算机科学, 98.5, 软件工程(97), 计算机组成原理(100)
王杨, 人工智能, 98, 操作系统原理(98), 人工智能导论(98)
杜云, 计算机科学, 98, 离散数学(100), 编译原理(100), 数据结构与算法(94)
张超, 人工智能, 98, 数据库系统(97), 程序设计基础(99)
黄秀荣, 人工智能, 97.8, 编译原理(94), 计算机组成原理(96), 操作系统原理(100), 计算机网络(100), 人工智能导论(99)
邵建华, 计算机科学, 97.67, 离散数学(97), 人工智能导论(99), 数据结构与算法(97)
张红霞, 人工智能, 97.5, 计算机组成原理(96), 编译原理(99)
张想, 计算机科学, 97.5, 编译原理(99), 操作系统原理(96)
李佳, 计算机科学, 96.67, 数据结构与算法(96), 人工智能导论(94), 程序设计基础(100)
秦斌, 计算机科学, 96.67, 程序设计基础(92), 数据库系统(100), 软件工程(98)
潘红梅, 人工智能, 96, 数据库系统(98), 操作系统原理(99), 程序设计基础(91)
盛杨, 计算机科学, 96, 操作系统原理(98), 软件工程(94)
黎金凤, 计算机科学, 95.75, 程序设计基础(93), 计算机组成原理(96), 数据结构与算法(100), 离散数学(94)
许宇, 人工智能, 95.75, 离散数学(95), 计算机组成原理(98), 数据库系统(92), 程序设计基础(98)
许春梅, 计算机科学, 95.5, 计算机组成原理(99), 操作系统原理(92)
刘凯, 计算机科学, 95.5, 软件工程(95), 人工智能导论(96)
王建国, 计算机科学, 95.5, 操作系统原理(100), 编译原理(91)
周淑珍, 人工智能, 95.5, 程序设计基础(96), 软件工程(95)
张宁, 人工智能, 95.33, 离散数学(98), 编译原理(97), 操作系统原理(91)
赵晶, 人工智能, 95.25, 程序设计基础(91), 离散数学(98), 编译原理(97), 计算机网络(95)
胡雪, 计算机科学, 95, 软件工程(93), 人工智能导论(97)
王华, 计算机科学, 95, 数据结构与算法(94), 计算机组成原理(96)
韩旭, 人工智能, 94.75, 数据库系统(93), 操作系统原理(98), 数据结构与算法(91), 离散数学(97)
赵建国, 人工智能, 94, 操作系统原理(92), 软件工程(94), 离散数学(96)
邹淑珍, 计算机科学, 93.5, 数据库系统(92), 计算机组成原理(95)
刘倩, 计算机科学, 93.5, 计算机网络(94), 离散数学(93)
杜帅, 计算机科学, 93.5, 计算机组成原理(93), 数据库系统(94)
史凯, 人工智能, 93.5, 数据库系统(93), 离散数学(94)
李雷, 计算机科学, 93, 操作系统原理(96), 数据结构与算法(91), 计算机组成原理(92)
周岩, 人工智能, 92, 程序设计基础(91), 数据库系统(93)
朱龙, 人工智能, 92, 操作系统原理(91), 编译原理(93)
廖刚, 计算机科学, 91, 计算机组成原理(91), 离散数学(91)
```

MCP应用在执行较为复杂的数据库操作时可能会出现无法正确使用工具的情况，一方面可能是MCP工具本身设计得不够好，另一方面可能是LLM缺少相关领域内最新的知识。另外，在第一次执行失败后LLM尝试分步执行查询，但是却并没有正确地生成工具调用，而是生成了一份执行方案，直到用户发出新的消息才继续开始执行方案。

## 5. 当前面临的问题与解决思路

在实际测试中，我发现我的MCP客户端系统面临几个关键挑战，下面我将详细分析这些问题并提出可能的解决方案。

### 5.1 问题1：工具信息过载

#### 5.1.1 问题描述
当连接的工具服务器增多，可用工具数量变多时，系统会将所有工具描述都添加到对话上下文中，这会:
1. 占用大量的上下文窗口空间
2. 分散模型的注意力，导致模型可能忽略用户的实际需求
3. 降低模型对真正相关工具的选择准确率

问题的关键代码在`get_available_tools()`方法中：
```python
# 收集工具信息用于更新对话管理器的工具上下文
tool_info = f"- {tool.name}: {tool.description}"
tools_info.append(tool_info)

# 更新对话管理器中的工具上下文
if tools_info:
    self.conversation_manager.update_tool_context("\n".join(tools_info))
```

#### 5.1.2 可能的解决思路
1. **工具分类与按需加载**：对工具进行分类，只在需要时加载特定类别的工具。可以实现一个简单的关键词匹配系统，根据用户查询预筛选相关工具。

2. **工具描述压缩**：简化工具描述，只保留核心功能说明，减少每个工具占用的token数量。

3. **工具推荐系统**：构建一个简单的工具推荐系统，根据用户查询历史和当前查询，预测可能需要的工具，只提供这些工具的信息。

4. **分层工具访问**：首先只提供工具类别信息，当模型确定需要某类工具时，再提供该类别下的具体工具详情。可以修改代码为：
```python
# 先提供工具类别概述
tool_categories = self.group_tools_by_category()
self.conversation_manager.update_tool_context(tool_categories)

# 当LLM请求某类工具时再提供详细信息
async def provide_category_tools(self, category):
    category_tools = self.get_tools_by_category(category)
    self.conversation_manager.update_tool_context(category_tools)
```

### 5.2 问题2：上下文窗口溢出

#### 5.2.1 问题描述
在处理复杂任务时，系统需要进行多次工具调用，每次都会将调用结果添加到对话历史中。这导致:
1. 历史记录快速增长，很容易超出模型的上下文窗口限制
2. 即使使用了`optimize_history()`方法，由于许多消息被标记为关键消息（`is_key_message=True`），优化效果有限
3. 对话进行到一定程度后，新的工具调用结果可能被截断或丢失

问题的关键代码在`process_tool_result()`方法中：
```python
# 添加工具调用结果到消息历史，并标记为关键消息
tool_result_msg = {
    "role": "tool",
    "name": tool_name,
    "content": str(tool_content)
}
self.conversation_manager.add_message(tool_result_msg, is_key_message=True)
```

#### 5.2.2 可能的解决思路
1. **选择性标记关键消息**：不是所有工具调用结果都同样重要，可以根据内容长度、相关性等因素，动态决定是否将某个工具结果标记为关键消息。

2. **历史压缩与摘要**：定期对历史消息进行压缩和摘要，尤其是工具调用结果，保留关键信息的同时减少token占用。可以开发一个摘要函数：
```python
async def summarize_tool_results(self):
    # 找出所有工具调用结果
    tool_messages = [msg for msg in self.conversation_manager.messages 
                     if msg.get("role") == "tool"]
    # 按工具类型分组
    grouped_results = self.group_by_tool_type(tool_messages)
    # 为每组生成摘要
    summarized_results = []
    for tool_type, messages in grouped_results.items():
        summary = await self.generate_summary(messages)
        summarized_results.append({
            "role": "system",
            "content": f"{tool_type} 工具调用结果摘要: {summary}",
            "is_key_message": True
        })
    # 替换原始消息
    self.conversation_manager.replace_messages(tool_messages, summarized_results)
```

3. **结果过滤**：对工具返回的大量结果进行过滤，只保留最相关的部分。这可以通过关键词提取或使用LLM进行内容提炼来实现。

4. **分阶段对话管理**：将任务执行划分为不同阶段，每个阶段结束后，对该阶段的对话历史进行摘要处理，只保留结论部分，减少后续阶段的历史负担。

### 5.3 问题3：工具调用格式问题

#### 5.3.1 问题描述
有时模型不会返回标准的工具调用格式，或者在应该返回工具调用的情况下返回了文本回答。具体表现为:
1. 模型直接生成文本回答，而不是使用工具调用格式
2. 模型使用不规范的工具调用格式，导致解析失败
3. 模型在需要多步工具调用时过早地给出结论

这个问题不是代码实现问题，而是模型行为问题，通常与前两个问题相关，在工具信息过载和上下文窗口溢出的情况下更容易发生。

#### 5.3.2 可能的解决思路
1. **强化系统提示**：在系统提示中更明确地指示模型应该何时使用工具调用以及如何正确格式化工具调用。可以修改初始系统提示：
```python
self.conversation_manager.add_message ({
    "role": "system",
    "content": """
    你是一个专业的工具调用助手。请按照以下流程处理用户请求：
    1. 仔细分析用户需求
    2. 判断是否需要使用工具来完成任务
    3. 如果需要，必须使用标准工具调用格式（"tool_calls"字段）
    4. 分析工具结果后，决定是否需要进一步工具调用
    5. 只有在完全收集足够信息后，才生成最终回答
    
    重要提示：不要跳过工具调用直接回答；不要使用非标准格式调用工具；多步任务必须一步一步执行。
    """
})
```

2. **工具调用样例**：在系统提示中提供正确的工具调用样例，帮助模型理解预期的格式：
```python
self.conversation_manager.add_message({
    "role": "system",
    "content": """
    以下是正确的工具调用格式示例：
    {
        "tool_calls": [
            {
                "function": {
                    "name": "server.tool_name",
                    "arguments": {"param1": "value1", "param2": "value2"}
                }
            }
        ]
    }
    
    务必使用这种格式进行工具调用。
    """
})
```

3. **强化结果审查提示**：增强工具结果审查阶段的系统提示，明确引导模型继续使用工具而不是直接生成回答：
```python
temp_system_msg = {
    "role": "system-tip",
    "content": """
    请审查最近的用户消息和工具结果。此时有两种选择：
    1. 如果现有信息不足以完全解决用户问题，必须继续调用工具，请使用标准工具调用格式
    2. 只有当你确信已收集所有必要信息时，才可以生成最终回答
    
    警告：不要过早下结论；不要在信息不完整时生成方案；如有任何不确定，继续使用工具收集信息。
    """
}
```

## 6. MCP客户端的实际工作流程示例

为了更好地理解MCP客户端的工作流程，我们可以看一个数据库操作的具体例子：

1. **用户输入**: "查询数据库中销售额超过10万的客户，并发送一封感谢邮件"
2. **客户端处理**:
   - 将用户输入添加到对话历史
   - 获取所有服务器上的可用工具（包括数据库查询工具、邮件发送工具等）
   - 优化对话历史，确保在令牌限制内
3. **LLM决策**:
   - 客户端将用户输入和工具信息发送给LLM
   - LLM分析后决定需要先调用"数据库.执行查询"工具
4. **工具调用**:
   - 客户端解析工具调用请求，得到SQL查询语句
   - 找到数据库服务器和查询工具
   - 执行数据库查询，获取符合条件的客户列表
5. **结果处理**:
   - 将查询结果（可能是一个很长的客户列表）添加到对话历史
   - 将结果发送回LLM
6. **二次决策**:
   - LLM分析客户列表，决定需要使用"邮件.批量发送"工具
   - LLM生成邮件模板和收件人列表
7. **再次工具调用**:
   - 客户端执行"邮件.批量发送"工具
   - 获取邮件发送状态和结果
8. **最终响应**:
   - LLM分析所有结果，生成最终回答（例如："已成功向37位高价值客户发送了感谢邮件"）
   - 客户端将回答呈现给用户

实际工作中，这个流程中可能会出现我们前面讨论的三个问题：
1. 一开始工具信息太多，模型可能难以从众多工具中正确选择"数据库.执行查询"工具
2. 中间过程中，数据库查询结果可能包含大量客户数据，导致上下文窗口接近溢出
3. 模型可能在看到客户列表后，没有使用正确的工具调用格式来调用邮件工具，而是直接描述"应该发送邮件给这些客户"

## 7. 总结

这个MCP客户端基于ModelContextProtocol协议，实现了LLM与工具服务器之间的协作。它的关键特点包括：

1. **流程驱动设计**：整个系统围绕"用户输入→LLM决策→工具调用→结果处理→二次决策→最终响应"的流程设计，每个组件负责流程中的特定部分。

2. **协议适配**：通过ToolAdapter类，将MCP协议定义的工具格式转换为LLM可理解的格式，实现了协议之间的无缝转换。

3. **迭代工具调用**：实现了一种迭代的工具调用流程，允许LLM进行多步骤推理，使用多个工具来解决复杂问题。

4. **对话管理**：通过ConversationManager类，实现了对话历史的智能管理和优化，解决了LLM上下文窗口有限的问题。

5. **异步实现**：整个系统采用异步编程实现，提高了系统的响应速度和资源利用率。

对于我目前面临的三个问题，总结一下解决方案：

1. **工具信息过载问题**：实现工具分类与按需加载、工具描述压缩、或分层工具访问。工具信息只提供关键必要部分，减少模型的认知负担。

2. **上下文窗口溢出问题**：采用选择性标记关键消息、历史压缩与摘要、结果过滤、分阶段对话管理等方法，降低历史记录占用的空间，保留最有价值的信息。

3. **工具调用格式问题**：通过强化系统提示、提供工具调用样例、增强结果审查提示以及引入中间检查点等方法，引导模型正确使用工具调用格式，遵循预期的推理流程。

